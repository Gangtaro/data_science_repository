{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split, ParameterGrid\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, f1_score\n",
    "from sklearn.datasets import load_iris, load_digits, load_boston\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rng = np.random.RandomState(950530)\n",
    "lgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "y = iris['target']\n",
    "X = iris['data']\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMClassifier(n_jobs=1)\n",
    "lgb_model.fit(X[train_index], y[train_index])\n",
    "predictions = lgb_model.predict(X[test_index])\n",
    "actuals = y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(max_cat_group=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.LGBMClassifier(max_cat_group = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  0  0]\n",
      " [ 0 50  0]\n",
      " [ 0  0 50]]\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier(max_cat_threshold = 1)\n",
    "lgb_model.fit(X, y)\n",
    "predictions = lgb_model.predict(X)\n",
    "actuals = y\n",
    "print(confusion_matrix(actuals, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 정리\n",
    "**파라미터 참고문서**\n",
    "- [lightGBM parameters](https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective)\n",
    "- [Laura++](https://sites.google.com/view/lauraepp/parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 파라미터 ####\n",
    "\n",
    "# 최대 깊이\n",
    "max_depth = -1\n",
    "\n",
    "# 하나의 Leaf가 가지고 있는 최소한의 관측치 수\n",
    "min_data_in_leaf = 20\n",
    "\n",
    "# column sampling ratio\n",
    "feature_fraction = 1 # .7 (typically)\n",
    "\n",
    "# row sampling ratio\n",
    "bagging_fraction = 1 # .7 (typically)\n",
    "\n",
    "# ratio of L1, L2 regularization\n",
    "lambda_l1 = 0\n",
    "lambda_l2 = 0\n",
    "\n",
    "# 범주형 변수가 있는 경우, 범주의 최대 개수 설정\n",
    "max_cat_threshold = 32\n",
    "\n",
    "# \n",
    "early_stopping_round = 'NULL'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Core parameter ####\n",
    "#### ############## ####\n",
    "\n",
    "task = 'train'\n",
    "#  'train'         , for training, \n",
    "#  'predict'       , for prediction, \n",
    "#  'convert_model' , for converting model file into if-else format, see more information in Convert Parameters\n",
    "#  'refit'         , for refitting existing models with new data, aliases: refit_tree\n",
    "#  'save_binary'   , load train (and validation) data then save dataset to binary file. Typical usage: save_binary first, then run multiple train tasks in parallel using the saved binary file\n",
    "# Note: can be used only in CLI version; for language-specific packages you can use the correspondent functions\n",
    "\n",
    "boosting = 'gdbt'\n",
    "#  'gbdt' , traditional Gradient Boosting Decision Tree, aliases: gbrt\n",
    "#  'rf'   , Random Forest, aliases: random_forest\n",
    "#  'dart' , Dropouts meet Multiple Additive Regression Trees\n",
    "#  'goss' , Gradient-based One-Side Sampling\n",
    "# Note: internally, LightGBM uses gbdt mode for the first 1 / learning_rate iterations\n",
    "\n",
    "application = 'regression'\n",
    "# objective\n",
    "# Go README.md\n",
    "\n",
    "num_iterations = 100 \n",
    "# typically n_iter >= 100\n",
    "# number of boosting iterations\n",
    "\n",
    "learning_rate = 0.1 \n",
    "# typically 0.1, 0.001, 0.0003\n",
    "# Go README.md\n",
    "\n",
    "num_leaves = 31 \n",
    "# max number of leaves in one tree\n",
    "\n",
    "\n",
    "\n",
    "#### Metric parameters ####\n",
    "#### ################# ####\n",
    "\n",
    "metric = ''\n",
    "# 'mean_absolute_error',     MAE\n",
    "# 'mean_squared_error',      MSE\n",
    "# 'root_mean_squared_error', RMSE\n",
    "# 'binary_logloss'\n",
    "# 'multi_logloss'\n",
    "# For more detail, Go README.md\n",
    "\n",
    "\n",
    "#### I/O parameters ####\n",
    "#### ############# ####\n",
    "\n",
    "max_bin = 32\n",
    "\n",
    "categorical_feature = ''\n",
    "# 범주형 변수의 index를 기입\n",
    "# categorical_feature = 0,1,2\n",
    "# categorical_feature = name: C0, C1, C2\n",
    "\n",
    "ignore_column = ''\n",
    "# 학습에서 무시할 변수 선택\n",
    "# 위와 동일한 방법 사용\n",
    "\n",
    "\n",
    "\n",
    "PARAMETERS = {\n",
    "    'max_depth' : max_depth,\n",
    "    'min_data_in_leaf' : min_data_in_leaf,\n",
    "    'feature_fraction' : feature_fraction,\n",
    "    'bagging_fraction' : bagging_fraction,\n",
    "    'early_stopping_round' : early_stopping_round,\n",
    "    'lambda_l1' : lambda_l1,\n",
    "    'lambda_l2' : lambda_l2,\n",
    "    'max_cat_threshold' : max_cat_threshold,\n",
    "    \n",
    "    'task' : task,\n",
    "    'boosting' : boosting,\n",
    "    'application' : application,\n",
    "    'num_iterations' : num_iterations,\n",
    "    'learning_rate' : learning_rate,\n",
    "    \n",
    "    'metric' : metric,\n",
    "    \n",
    "    'max_bin' : max_bin,\n",
    "    'categorical_feature' : categorical_feature\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22  0  0]\n",
      " [ 6 20  0]\n",
      " [ 0  4 23]]\n",
      "[[22  6  0]\n",
      " [ 0 20  4]\n",
      " [ 0  0 23]]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(**parameters)\n",
    "    lgb_model.fit(X[train_index], y[train_index])\n",
    "    predictions = lgb_model.predict(X[train_index])\n",
    "    actuals = y[test_index]\n",
    "    print(confusion_matrix(actuals, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
