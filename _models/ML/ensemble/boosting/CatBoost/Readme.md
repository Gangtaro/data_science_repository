# CatBoost

- Gradient Boosting 기반한 알고리즘 (ex. LightGBM, XGBM, ...)
- 다른 gbm 보다 사용하기 쉽고, 좋은 결과도 나타나는 경우가 있다.
- Symmetric Tree를 사용하기 때문에, 모델 학습시에는 오랜시간이 소요되어도, 예측시에는 몇 십배 빠른 속도를 가진다.


- Gradient Boosting 방법에 기반한 알고리즘이지만, 약간 다른 방식을 취하고 있다.
    - General algorithm of Gradient Boosting
        1. 모든 데이터 셋에 대하여 잔차를 구하며 학습하여 편의가 큰 모델을 만든다.
        2. 이전에 학습한 모델로 부터 계산된 잔차를 통해 새로운 모델을 만든다.
        3. 일정 시점까지 위를 반복
        
    - CatBoosting Algoritm
        1. 모든 시점에 대하여 잔차를 구하는데 적용하는 모델은 그 시점의 데이터에 대한 정보를 포함하지 않는다. 
        2. 나온 잔차를 통해 모델을 학습한다.
        3. 일정 시점까지 위를 반복
        
- 데이터의 크기가 클 수록 엄청난 계산 수를 요하는 작업이다.
- 이런 방식의 알고리즘을 Ordered boosting이라고 부른다.
- 이런 순차적인 알고리즘 방법 때문에 변수의 순서가 중요하다. 따라서 모델 내에서 여러가지 조합의 무작위 순열로 바꾸어주는 기능이 내장되어있다.(Ramdom Permutation) 기본적으로 아무런 세팅을 하지 않았다면 4가지 무작위 순열이 생성될것이다. 이는 오버피팅을 방지해줄 것이며, ```bagging_temperature``` 라는 파라미터를 통해서 조정가능하다. 

- categorical feature handling
    - 몇 가지 다른 GBM과는 달리 범주형 변수를 따로 원핫인코딩을 하지 않아도 된다. 내장되어있는 인코딩 함수가 있기때문. 다음과 같은 방식의 새로운 접근법으로 인코딩한다. Ordered Target encoding.
    - Ordered boosting과 유사한 방법으로, 현 시점의 데이터를 인코딩하기 위해 이전에 사용한 데이터들의 평균 값으로 인코딩한다. 이 방법은 현 시점의 데이터에 대한 Target value의 정보가 포함되지 않아 Data leakage 문제를 일으키지 않는다는 장점과 다양한 값을 나타 낼 수 있다는 장점이 있다.
    
    - 기본적으로 2개의 범주를 가지는 범주형 변수는 OneHotEncoding을 해준다. 이는 ```one_hot_max_size``` 로 최대 개수를 정해서 원핫인코딩할 범주형 변수의 기준을 만들어 줄 수 있다.
    
    
- 단점
    - sparse matrix를 학습하기 위한 기능을 제공하지 않는다.
    - LightGBM보다 학습에 많은 시간이 소요된다.
    
    
- CatBoost를 사용할만한 여러가지 상황  
    하이퍼 파라미터 튜닝은 Catboost에서 크게 중요한 측면은 아니지만, 풀어야할 문제에 대해서 올바른 파라미터를 설정하는 것은 매우 중요하다. 다음과 같은 상황이 있다.
    1. 시간의 흐름이 있는 데이터  
        시계열 데이터를 학습시키기 위해서는 ```has_time = True```으로 설정해준다.
    2. 예측 시간의 지연이 적은 곳  
        XGBM의 예측보다 무려 8배나 빠르다.
    3. 