{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target(label) 변수의 특성에 따라 사용해야하는 모델이 다르고 (1-3)  \n",
    "Parameter optimization 하는 방법에 따라 조금씩 다르다.\n",
    "\n",
    "1. **binary classification** --> use XGBClassifier\n",
    "2. **multi-class classification** -> use XGBClassifier\n",
    "3. **Regression** --> use XGBRegressor\n",
    "4. **Parameter optimization** --> use XGBRegressor\n",
    "\n",
    "SEE! [sklearn - XGBoost Parameters](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)\n",
    "\n",
    "---\n",
    "참고.  \n",
    "- **XGBClassifier :** Lable이 **이항변수, 다항변수**\n",
    "- **XGBRegressor  :** Lable이 **연속형 자료**\n",
    "\n",
    "# import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.3'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Created on 1 Apr 2015\n",
    "@author: Jamie Hall\n",
    "'''\n",
    "\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, f1_score\n",
    "from sklearn.datasets import load_iris, load_digits, load_boston\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rng = np.random.RandomState(31337)\n",
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. binary classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros and Ones from the Digits dataset: binary classification\n"
     ]
    }
   ],
   "source": [
    "print(\"Zeros and Ones from the Digits dataset: binary classification\")\n",
    "digits = load_digits(n_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X :  (360, 64)\n",
      "Shape of y :  (360,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X : ', digits.data.shape)\n",
    "print('Shape of y : ', digits.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = digits['target'] # label\n",
    "X = digits['data']\n",
    "\n",
    "# make K-fold cross validation instance + you can use \n",
    "kf = KFold(n_splits=2,       # there is 2 folds\n",
    "           shuffle=True,     # 데이터를 분할하기 전에 섞어줘\n",
    "           random_state=rng) # seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KFold (k-folds cross validation)** See also.  \n",
    "\n",
    "\n",
    "- [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)  \n",
    "Takes group information into account to avoid building folds with imbalanced class distributions (for binary or multiclass classification tasks).\n",
    "- [GroupKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold)  \n",
    "K-fold iterator variant with non-overlapping groups.\n",
    "- [RepeatedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html#sklearn.model_selection.RepeatedKFold)  \n",
    "Repeats K-Fold n times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:45:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[93  1]\n",
      " [ 0 86]]\n",
      "[00:45:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[84  0]\n",
      " [ 2 94]]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(n_jobs=1, use_label_encoder=False).fit(X[train_index], y[train_index])\n",
    "    predictions = xgb_model.predict(X[test_index])\n",
    "    actuals = y[test_index]\n",
    "    \n",
    "    print(confusion_matrix(actuals, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. multi-class classification\n",
    "## load the data and transfer to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris: multiclass classification\n"
     ]
    }
   ],
   "source": [
    "print(\"Iris: multiclass classification\")\n",
    "iris = load_iris()\n",
    "y = iris['target']\n",
    "X = iris['data']\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[24  0  0]\n",
      " [ 0 22  4]\n",
      " [ 0  0 25]]\n",
      "[00:53:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[26  0  0]\n",
      " [ 0 23  1]\n",
      " [ 0  2 23]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gangtaro/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/gangtaro/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(n_jobs=1).fit(X[train_index], y[train_index])\n",
    "    predictions = xgb_model.predict(X[test_index])\n",
    "    actuals = y[test_index]\n",
    "    \n",
    "    print(confusion_matrix(actuals, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regression\n",
    "## load the data and transfer to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston Housing: regression\n"
     ]
    }
   ],
   "source": [
    "print(\"Boston Housing: regression\")\n",
    "boston = load_boston()\n",
    "y = boston['target']\n",
    "X = boston['data']\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.241413840538742\n",
      "15.104799766676079\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    xgb_model = xgb.XGBRegressor(n_jobs=1).fit(X[train_index], y[train_index])\n",
    "    predictions = xgb_model.predict(X[test_index])\n",
    "    actuals = y[test_index]\n",
    "    print(mean_squared_error(actuals, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Parameter Opimization\n",
    "- **n_estimators(int) :**  Number of gradient boosted trees. Equivalent to number of boosting rounds\n",
    "- **max_depth(int) :** Maximum tree depth for base learners\n",
    "\n",
    "## using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter optimization\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "0.6839859272017424\n",
      "{'max_depth': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter optimization01 : GridSearchCV\")\n",
    "y = boston['target']\n",
    "X = boston['data']\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_jobs=1)\n",
    "clf = GridSearchCV(xgb_model,\n",
    "                   {'max_depth': [2, 4, 6],\n",
    "                    'n_estimators': [50, 100, 200]}, verbose=1, n_jobs=1)\n",
    "\n",
    "clf.fit(X, y)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using ParameterGrid (***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter optimization01 : ParameterGrid\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter optimization01 : ParameterGrid\")\n",
    "\n",
    "y = boston['target']\n",
    "X = boston['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With K-fold Cross validation\n",
    "\n",
    "+there is another way to CrossValidation → [Many kind of CV]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold, Shuffle\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1st way]  CV --> Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 2, 'n_estimators': 183} 7.966413949080774\n",
      "{'max_depth': 3, 'n_estimators': 112} 6.451319132204588\n",
      "{'max_depth': 5, 'n_estimators': 75} 7.804371671162519\n",
      "{'max_depth': 5, 'n_estimators': 95} 10.340803882852418\n",
      "{'max_depth': 5, 'n_estimators': 77} 16.58959935221183\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 그리드 설정\n",
    "XGB_parameter_grid = ParameterGrid({\"max_depth\": np.arange(2, 6),\n",
    "                                  \"n_estimators\": np.arange(50, 200)})\n",
    "\n",
    "for train_index, test_index in kf.split(X):  \n",
    "\n",
    "    best_score = 1000\n",
    "    for parameter in XGB_parameter_grid:\n",
    "\n",
    "        model = xgb.XGBRegressor(n_jobs =1, verbosity = 1, **parameter).fit(X[train_index], y[train_index])\n",
    "        pred_Y = model.predict(X[test_index])\n",
    "        score = mean_squared_error(pred_Y, y[test_index])\n",
    "\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_parameter = parameter\n",
    "        \n",
    "    print(best_parameter, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2nd way] Parameter 설정 --> CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter :  {'max_depth': 5, 'n_estimators': 199}\n",
      "Best Score(MSE): 9.488505972585271\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 그리드 설정\n",
    "XGB_parameter_grid = ParameterGrid({\"max_depth\": np.arange(2, 6),\n",
    "                                  \"n_estimators\": np.arange(50, 200)})\n",
    "  \n",
    "# [1st Loop]\n",
    "# Set parameter which we want test\n",
    "for parameter in XGB_parameter_grid:\n",
    "\n",
    "    best_score = 1000\n",
    "    avr_score = 0\n",
    "    _scores = []\n",
    "    \n",
    "    # [2nd Loop]\n",
    "    # K-fold cross validation -> Mean Score is the 'Set of parameters's score\n",
    "    for train_index, test_index in kf.split(X):\n",
    "\n",
    "        model = xgb.XGBRegressor(n_jobs =1, verbosity = 1, **parameter)\n",
    "        model.fit(X[train_index], y[train_index],\n",
    "                 verbose = 1)\n",
    "        pred_Y = model.predict(X[test_index])\n",
    "        score = mean_squared_error(pred_Y, y[test_index])\n",
    "        \n",
    "        _scores.append(score)\n",
    "\n",
    "    avr_score = np.mean(_scores)\n",
    "\n",
    "    if avr_score < best_score:\n",
    "        best_score = avr_score\n",
    "        best_parameter = parameter\n",
    "    \n",
    "print('Best Parameter : ', best_parameter)\n",
    "print('Best Score(MSE):', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Parameter를 설정한 객체 저장\n",
    "# By 'Pickle'\n",
    "\n",
    "# Best Model\n",
    "best_boston_XGBM = xgb.XGBRegressor(n_jobs =1, verbosity = 1, **best_parameter).fit(X, y)\n",
    "\n",
    "# Pickling - dump\n",
    "pickle.dump(best_boston_XGBM, open(\"best_boston_XGBM.pkl\", \"wb\"))\n",
    "\n",
    "# Pickling - load\n",
    "Can_use_this_model = pickle.load(open(\"best_boston_XGBM.pkl\", \"rb\"))\n",
    "Can_use_this_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=5,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=199, n_jobs=1, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Can_use_this_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using just split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, y, random_state = rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'n_estimators': 64} 14.224654335528523\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 그리드 설정\n",
    "XGB_parameter_grid = ParameterGrid({\"max_depth\": np.arange(2, 6),\n",
    "                                  \"n_estimators\": np.arange(50, 200)})\n",
    "\n",
    "best_score = 1000\n",
    "\n",
    "for parameter in XGB_parameter_grid:\n",
    "\n",
    "    model = xgb.XGBRegressor(n_jobs =1, verbosity = 1 , **parameter).fit(Train_X, Train_Y)\n",
    "    pred_Y = model.predict(Test_X)\n",
    "    score = mean_squared_error(pred_Y, Test_Y)\n",
    "\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_parameter = parameter\n",
    "\n",
    "print(best_parameter, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix.\n",
    "\n",
    "## 1. Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.99950\n",
      "[1]\tvalidation_0-auc:0.99975\n",
      "[2]\tvalidation_0-auc:0.99975\n",
      "[3]\tvalidation_0-auc:0.99975\n",
      "[4]\tvalidation_0-auc:0.99975\n",
      "[5]\tvalidation_0-auc:0.99975\n",
      "[6]\tvalidation_0-auc:1.00000\n",
      "[7]\tvalidation_0-auc:1.00000\n",
      "[8]\tvalidation_0-auc:1.00000\n",
      "[9]\tvalidation_0-auc:1.00000\n",
      "[10]\tvalidation_0-auc:1.00000\n",
      "[11]\tvalidation_0-auc:1.00000\n",
      "[12]\tvalidation_0-auc:1.00000\n",
      "[13]\tvalidation_0-auc:1.00000\n",
      "[14]\tvalidation_0-auc:1.00000\n",
      "[15]\tvalidation_0-auc:1.00000\n",
      "[16]\tvalidation_0-auc:1.00000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=1, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early-stopping\n",
    "# 내부 파라미터를 이용해서 (evaluation parameter) Early Stop 기능 활성화 가능\n",
    "\n",
    "# data from 'digits'\n",
    "X = digits['data']\n",
    "y = digits['target']\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "clf = xgb.XGBClassifier(n_jobs=1)\n",
    "clf.fit(X_train, y_train, \n",
    "        early_stopping_rounds=10, \n",
    "        eval_metric=\"auc\",\n",
    "        eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pickle\n",
    "can use pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling sklearn API models\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# The sklearn API models are picklable\n",
    "print(\"Pickling sklearn API models\")\n",
    "\n",
    "# must open in binary format to pickle\n",
    "pickle.dump(clf, open(\"best_boston.pkl\", \"wb\"))\n",
    "clf2 = pickle.load(open(\"best_boston.pkl\", \"rb\"))\n",
    "print(np.allclose(clf.predict(X), clf2.predict(X)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "311.64404296875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
