{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 대회 설명\n",
    "\n",
    "- 신용카드 사용자들의 개인 신상정보 데이터로 **사용자의 신용카드 대금 연체 정도를 예측**\n",
    "- log loss 점수를 최소화하라.\n",
    "\n",
    "---\n",
    "- 참여목적: \n",
    "    - **lightGBM과 Xgboost 모델**의 사용법을 익힌 후, 해당 모델을 실제 데이터 분석 사례에 적용해보고 싶어서 참가.\n",
    "    - Credit Card **FDS**(Fraud Detection System)에 대한 관심도가 생겼는데, 해당 주제의 프로젝트의 과제와 문제 해결 방안에 대해서 알아보고 싶어서 참가.\n",
    "    - 이 프로젝트를 통해 신용카드 관련 데이터분석에서 어떠한 변수가 학습모델에 유의하는지 파악할 기회를 가지고 인사이트를 확보한다.\n",
    "    \n",
    "- 느낀점:  \n",
    "    - **잘못설계된 파생 변수는 모델 성능에 아주 큰 영향을 끼친다.**  \n",
    "        여러가지 방법으로 성능 향상 시도를 해보았지만, 성능 향상에 한계를 느꼈던 부분이 있었다. 돌파구 마련을 위해서 생각해낸 방법은 **프로젝트의 EDA부터 성능 평가까지 모든 과정을 검토**하는 것이였다. 확실한 문제점이 있었다. '개인식별코드'에 관련한 변수로, 해당 변수를 Count 한 값을 '소유하고 있는 카드의 개수'라는 변수를 만들고 이를통해 파생의 파생 변수들도 생성해냈었는데, 이것에 심각한 오류를 발견했다. Train data만을 기준으로 변수를 설계할때는 인지하지 못했지만, Test data에 카드 사용자가 Train data에도 있다는 사실을 인지하게 되었다. 따라서 A라는 카드 사용자가 Train data에서는 3개의 카드를 소유하고 있다고 하지만, test data에서는 2개의 카드를 소유하고 있다고 하는 것과 같은 심각한 정보의 오류를 범하고 말았다.  \n",
    "        이와 관련된 변수를 모두 제거해주고, 엄청난 성능의 향상을 만들었다. 잘못설계된 변수는 모델 성능 향상에 아주 악영향을 끼치게 했던 것이다.\n",
    "    \n",
    "    - **폭 넓지 못한  ML 선택지**  \n",
    "        결과적으로 알게된 사실인데, 이 데이터 셋을 학습하는데 가장 좋은 성능을 발휘하는 ML은 Catboost 였다는 것을 알게 되었다. 초기에 채택한 모델은 ```LGBM```과```XGBM``` 딱 두 가지 모델이였다.(```SVM```또한 학습시켜봤지만 터무니 없는 score로 배제하였다.) 하지만, 두 모델은 모두 내가 만든 데이터 셋에서 과적합되는 모습을 보였다.\n",
    "        많은 사람들이 Catboost 를 사용했었고, 해당 모델에 맞게 해당 프로젝트를 전체적으로 수정하였고, 기존 보다 더 나은 score를 기록할 수 있게 해주었다.\n",
    "    \n",
    "    - **불균형 데이터에 대한 몇가지 알려진 처리 방법이 있다.**\n",
    "        1. 소수의 데이터를 증가 시키는 방법 → 학습데이터에 심각한 오버피팅을 유발했다.\n",
    "        2. 다수의 데이터를 감소 시키는 방법 → 전체적인 성능이 떨어지게 되었다.\n",
    "        3. 훈련시 클래스에 가중치 조절 → 성능에 큰 변화를 주지 못했다.  \n",
    "       \n",
    "    이번 프로젝트에서는 데이터의 개수가 Feature의 개수에 비하여 큰 데이터가 아니었다. 이런 경우에는 불균형 데이터에 대응 하는 것이 제한적이라는 것을 알게되었다. 이러한 이유에 효과적인 대응 방안을 고안해내지 못했던것이 아쉽다.\n",
    "    \n",
    "    \n",
    "    \n",
    "- 고쳐나갈점 및 배운점:\n",
    "    - LightGBM과 XGBM이 꼭 모든 데이터에서 최고의 성능을 발휘하는 것은 아니다. 데이터의 특성에 따라 심한 과적합을 발생시키기도 한다.\n",
    "    - CatBoost도 아직 충분히 좋은 결과를 만들어내고 있다.\n",
    "    - 데이터 분석 및 예측 매뉴얼 :\n",
    "        1. 간단한 EDA\n",
    "        2. 최고 간결한 Data Preprocessing (handling Missing Values, Outliers, ...)\n",
    "        3. 모델 선택\n",
    "            1. 분석에 사용가능할만한 모든 학습 모델 파악\n",
    "            2. 타 모델보다 확연하게 좋은 결과를 나타내는 학습 모델 2-3개 간추리기\n",
    "        4. 선택한 모델에 맞고, 분석의 취지에 알맞는 Feature를 재 생성 및 편집하여 예측의 정확도를 올린다.\n",
    "    - Train data를 바탕으로 새로운 Feature을 생성할때, 그것이 초기에 계획한 의도가 Test set에도 유지가 되는지 확인하는 단계가 필요하다.\n",
    "        \n",
    "        \n",
    "- 결과 : \n",
    "    - public  최고점수 log loss : **0.66693** **(3등, 2021.07.04 기준)**, \n",
    "    - private 최고점수 log loss : **0.6581080742**  \n",
    "    ![ranking image](https://github.com/Gangtaro/data_science_repository/blob/main/COMPETETION_LOG/DACON_210405_credit/ranking.png \"2021. 07. 04 기준 public ranking\")\n",
    "\n",
    "- 교훈 : 냉장고에 있는 재료를 다 넣는다고 해서 맛있는 음식이 완성되지는 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
